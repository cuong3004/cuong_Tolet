{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown \n",
        "!pip install torchmetrics\n",
        "!gdown --id 1n-YeSOxjOOKyhaU1OE3Bqp_HmDm3pIqq"
      ],
      "metadata": {
        "id": "NvNKFCxQf77q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmFROqkXgGRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_iN6wq2bE9S2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eILM57TLyfx4"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import pandas as pd    \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/file.csv\")"
      ],
      "metadata": {
        "id": "5ZaeaK69yr2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(df)"
      ],
      "metadata": {
        "id": "vwS-X2EuywQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df[[\"TimeProcess\",\"People\", \"H\", \"Positive\", \"Negative\", \"NH3\"]]\n",
        "df_label = df[\"label\"].values.astype(np.long)"
      ],
      "metadata": {
        "id": "0ohUryHEyweF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.head()"
      ],
      "metadata": {
        "id": "pmZ48tToVQaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.values "
      ],
      "metadata": {
        "id": "7beLUwDtzu1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_data = df_data."
      ],
      "metadata": {
        "id": "7j7G7b5qzw07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.head()"
      ],
      "metadata": {
        "id": "uN40LSfaU6le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_label"
      ],
      "metadata": {
        "id": "uqK5WE2gUwpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = (df_data-df_data.std(0)) / df_data.mean(0)"
      ],
      "metadata": {
        "id": "r1G2zO4kz4xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv(\"data.csv\", index=False)"
      ],
      "metadata": {
        "id": "8wKLyjrRVCMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data.csv | head"
      ],
      "metadata": {
        "id": "HwFLerfMVoRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_data)"
      ],
      "metadata": {
        "id": "TMJl7e_Tm5nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_train, df_label_train = df_data[:-1848], df_label[:-1848]\n",
        "df_data_valid, df_label_valid = df_data[-1848:], df_label[-1848:]"
      ],
      "metadata": {
        "id": "EmkDl3MQm-32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(df_data_train), len(df_label_train)"
      ],
      "metadata": {
        "id": "PCtwj1W8rFlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_valid"
      ],
      "metadata": {
        "id": "zqBr7jit0CBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "class DataVS(Dataset):\n",
        "    def __init__(self, df, label):\n",
        "        self.df = df.values.astype(np.float32)\n",
        "        self.df = self.df.reshape((-1,12,6))\n",
        "        self.label = label.reshape((-1,12))\n",
        "        # print(self.df.shape)\n",
        "        # print(self.label.shape)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.df[idx][:,2:4])\n",
        "        y = self.label[idx][-1]\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "data_vs_train = DataVS(df_data_train, df_label_train)\n",
        "data_vs_valid = DataVS(df_data_valid, df_label_valid)"
      ],
      "metadata": {
        "id": "z7nbSpVl0Yd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, inptut_size, lstm_size, n_classes, num_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(inptut_size, lstm_size)\n",
        "        self.lstm = nn.RNN(\n",
        "            input_size=lstm_size,\n",
        "            hidden_size=lstm_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(lstm_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear(x))\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:,-1])\n",
        "        return x\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', \n",
        "                                                  classes=np.unique(data_vs_train.label[:,-1]), \n",
        "                                                  y=data_vs_train.label[:,-1])\n",
        "\n",
        "samples_weight = np.array([class_weights[t] for t in data_vs_train.label[:,-1]])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data_vs_train, 128, sampler=sampler)\n",
        "validloader = torch.utils.data.DataLoader(data_vs_valid, 128, shuffle=False)\n",
        "\n",
        "net = Net(2, 32, 3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "best_acc = 0\n",
        "\n",
        "net.to(\"cuda\")\n",
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "    running_acc = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0.0\n",
        "    net.train()\n",
        "    \n",
        "    for i, data in tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        # print(inputs.shape)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i == len(trainloader) -1:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss/ len(trainloader)}, acc {correct / total}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    running_acc = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0.0\n",
        "    net.eval()\n",
        "    for i, data in tqdm(enumerate(validloader, 0), total=len(validloader)):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i == len(validloader) -1:    # print every 2000 mini-batches\n",
        "            acc_1 = correct / total\n",
        "            if best_acc < acc_1:\n",
        "                print(\"Save checkpoint\")\n",
        "                torch.save(net, \"model.pt\")\n",
        "                best_acc = acc_1\n",
        "            \n",
        "            print(f'EVAL loss: {running_loss/ len(validloader)}, acc {acc_1}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "total = []\n",
        "correct = []\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "acc = Accuracy(\"multiclass\", average=\"macro\", num_classes=3)\n",
        "net = torch.load(\"model.pt\")\n",
        "net.cpu()\n",
        "net.eval()\n",
        "for i, data in tqdm(enumerate(validloader, 0), total=len(validloader)):\n",
        "    inputs, labels = data\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total.extend(labels.tolist())\n",
        "    correct.extend(predicted.tolist())\n",
        "\n",
        "print(\"ACC:\", acc(torch.tensor(correct),torch.tensor(total)))\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "oC9Np2Rv8LAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# RNN\n",
        "\n",
        "# #\n",
        "# tensor(0.6389) 6531\n",
        "# #\n",
        "# tensor(0.6514) 6563\n",
        "# #\n",
        "# (0.6067) 6627\n",
        "\n",
        "\n",
        "\n",
        "# LSTM\n",
        "# tensor(0.6611) 25539\n",
        "# ##\n",
        "#  25571 tensor(0.7096)\n",
        "# ##\n",
        "#  25635 tensor(0.6145)"
      ],
      "metadata": {
        "id": "JiA5-ALD9APP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}